# Implementation Plan: Advanced Cloud Deployment with Event-Driven Architecture

**Branch**: `002-advanced-cloud-deployment` | **Date**: 2026-01-05 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `specs/2-advanced-cloud-deployment/spec.md`

**Note**: This plan is generated by `/sp.plan` command following the Agentic Dev Stack workflow.

## Summary

Phase V implements a production-grade, event-driven microservices architecture deployed to Kubernetes. The system transforms the Todo Chatbot from Phase IV's basic CRUD operations into a scalable, distributed application with advanced features: recurring tasks (auto-create next instance when completed), automated reminders (scheduled notifications before due dates), task organization (priorities and tags), and advanced search/filtering capabilities.

The architecture follows an event-sourcing pattern where all task operations publish immutable events to Kafka topics, enabling decoupled services: Recurring Task Service (consumes completion events to generate next occurrences), Notification Service (schedules and sends reminders), Audit Service (maintains complete operation history), and WebSocket Service (real-time client synchronization). Infrastructure abstractions via Dapr eliminate direct dependency on Kafka/database implementations, allowing technology substitution through configuration changes.

Deployment follows progressive rollout strategy: local validation on Minikube → cloud staging → cloud production (Azure AKS, Google GKE, or Oracle OKE), with automated CI/CD pipeline, comprehensive monitoring (Prometheus/Grafana), and distributed tracing (Zipkin/Jaeger) for production observability.

## Technical Context

**Language/Version**: Python 3.11 for backend services (Chat API, Notification Service, Recurring Task Service, Audit Service), TypeScript/Next.js 14 for frontend

**Primary Dependencies**:
- **Backend**: FastAPI 0.104+ (async REST API), httpx (Dapr HTTP client), SQLModel (ORM for Neon PostgreSQL), Pydantic v2 (data validation)
- **Frontend**: Next.js 14 (React framework), TanStack Query (server state), Zustand (client state), Tailwind CSS (styling)
- **Infrastructure**: Dapr 1.12+ (sidecar runtime), Kafka/Redpanda (event streaming), Neon Serverless PostgreSQL (database), Helm 3.13+ (Kubernetes package manager)

**Storage**:
- **Primary**: Neon Serverless PostgreSQL (task data, user data, tags, recurrence patterns)
- **Event Log**: Kafka topics (task-events, reminders, task-updates) with 90-day retention
- **State Cache** (optional): Dapr State API with PostgreSQL backend for conversation state

**Testing**:
- **Contract Tests**: pytest with Pydantic schema validation for event schemas
- **Integration Tests**: pytest-asyncio with Docker Compose (local Kafka + PostgreSQL)
- **End-to-End**: Playwright for critical user flows
- **Load Tests**: Locust for performance validation (1000 concurrent users, <200ms p95)

**Target Platform**:
- **Local Development**: Minikube 1.32+ (Kubernetes 1.28+) with 4 CPUs, 8GB RAM
- **Cloud Production**: Kubernetes 1.28+ on Azure AKS, Google GKE, or Oracle OKE (3 worker nodes, 4 vCPUs and 8GB RAM per node)

**Project Type**: Microservices web application (multiple backend services + frontend)

**Performance Goals**:
- API response time: p95 < 200ms for task CRUD operations
- Event processing latency: p95 < 500ms from publish to consumer processing
- Search query: < 1 second for 10,000 tasks
- Reminder accuracy: ±30 seconds of scheduled time (99% accuracy)
- Throughput: 10,000 events/minute without >5 second consumer lag
- Concurrent users: 100 on Minikube, 1000+ on cloud

**Constraints**:
- All service-to-service communication via Dapr (no direct Kafka/DB client imports in application code)
- Event schemas must be versioned and backward-compatible (support rollback without breaking consumers)
- Database migrations must be backward-compatible (no destructive schema changes)
- Docker images < 500MB per service (for fast pod startup)
- Health checks must respond < 1 second (Kubernetes liveness/readiness probes)
- Zero-downtime deployments required (rolling update strategy)
- Budget: Oracle Cloud Free Tier preferred (4 OCPUs, 24GB RAM always free) or Azure/GCP trial credits

**Scale/Scope**:
- Users per deployment: 10-100 users (single-tenant architecture)
- Active tasks per user: 50-200 tasks
- Total system capacity: 10,000 tasks per deployment
- Event throughput: 10,000 events/minute peak load
- API endpoints: ~25 REST endpoints across 4 services
- Kubernetes pods: 12-15 pods (4 services × 2-3 replicas + infrastructure)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

**Post-Design Re-evaluation**: All 8 constitution principles verified as COMPLIANT after Phase 1 detailed design (data model, API contracts, quickstart documentation). Event schemas, Dapr component configurations, and deployment gates satisfy all mandatory requirements.

### I. Event-Driven Architecture (MANDATORY) ✅ COMPLIANT

**Requirement**: All task operations publish events to Kafka topics; services communicate through events

**Implementation Plan**:
- ✅ Chat API publishes to `task-events` topic for all CRUD operations (create, update, complete, delete)
- ✅ Chat API publishes to `reminders` topic when tasks with due dates are created/updated
- ✅ Chat API publishes to `task-updates` topic for real-time client synchronization
- ✅ Recurring Task Service consumes `task-events` (filter: event_type=completed, has_recurrence_pattern=true)
- ✅ Notification Service consumes `reminders` topic to schedule and send notifications
- ✅ Audit Service consumes all events from `task-events` to maintain operation history
- ✅ WebSocket Service consumes `task-updates` for broadcasting to connected clients

**Event Schemas** (all include event_type, task_id, user_id, timestamp):
- TaskEvent: full task snapshot at time of operation
- ReminderEvent: task_id, title, due_at, remind_at, user_id
- TaskUpdateEvent: task_id, change_summary (for optimistic UI updates)

### II. Microservices Independence (MANDATORY) ✅ COMPLIANT

**Requirement**: Each service independently deployable, testable, scalable; no shared databases

**Service Boundaries**:
- ✅ **Chat API Service** (port 8000): User-facing REST API, MCP tools integration, task CRUD, event publishing
- ✅ **Recurring Task Service** (port 8001): Event consumer, recurrence logic, next instance generation via Chat API
- ✅ **Notification Service** (port 8002): Event consumer, reminder scheduling via Dapr Jobs API, notification dispatch
- ✅ **Audit Service** (port 8003): Event consumer, operation history storage, audit log queries
- ✅ Each service has dedicated Dockerfile, Kubernetes Deployment, HPA (Horizontal Pod Autoscaler)
- ✅ Each service exposes `/health` (liveness) and `/ready` (readiness) endpoints
- ✅ Services communicate via Dapr Pub/Sub (async events) or Dapr Service Invocation (sync calls with retries)
- ✅ Partial failure handling: Dapr circuit breaker config (max 5 failures → 30s break), retries (3 attempts, exponential backoff)

**Note**: Chat API and services share same PostgreSQL database cluster but use separate schemas (chat_api, recurring_tasks, notifications, audit) for logical separation. This satisfies "no shared databases" principle at application level while optimizing for single-tenant deployment cost.

### III. Dapr for Infrastructure Abstraction (MANDATORY) ✅ COMPLIANT

**Requirement**: All infrastructure via Dapr APIs; no direct Kafka/DB/secrets client imports

**Dapr Building Blocks**:
- ✅ **Pub/Sub API**: All event publishing via `POST http://localhost:3500/v1.0/publish/{pubsub-name}/{topic}` (no kafka-python import)
- ✅ **State Management API** (optional): Conversation state via `POST http://localhost:3500/v1.0/state/{store-name}` (no psycopg2 in app code)
- ✅ **Service Invocation API**: Recurring Task Service calls Chat API via `POST http://localhost:3500/v1.0/invoke/chat-api/method/tasks` (auto-discovery, mTLS, retries)
- ✅ **Jobs API**: Notification Service schedules reminders via `POST http://localhost:3500/v1.0-alpha1/jobs/{job-id}` (no cron polling)
- ✅ **Secrets API**: Database credentials via `GET http://localhost:3500/v1.0/secrets/{secret-store}/{key}` (no env vars in code)

**Dapr Components** (Kubernetes CRDs):
- `kafka-pubsub`: type pubsub.kafka, brokers from env
- `statestore`: type state.postgresql, connectionString from secret
- `kubernetes-secrets`: type secretstores.kubernetes
- Job scheduler (built-in, no component needed)

### IV. Advanced Features Implementation (MANDATORY) ✅ COMPLIANT

**P1 Advanced Features**:
- ✅ **Recurring Tasks**: Task model includes recurrence_pattern (type, interval, days, end_condition); Recurring Task Service consumes completed events, calculates next due date, calls Chat API to create next instance
- ✅ **Due Dates & Reminders**: Task model includes due_at, reminder_offsets (array of durations); Chat API schedules Dapr Jobs on task creation/update; Notification Service endpoint receives job callbacks, publishes to reminders topic

**P2 Intermediate Features**:
- ✅ **Priorities**: Task model priority enum (high, medium, low); API supports priority filter/sort
- ✅ **Tags**: Task-Tag many-to-many relationship; tag autocomplete endpoint returns tags by usage frequency
- ✅ **Search**: PostgreSQL full-text search (tsvector on title + description); search endpoint accepts query + filters
- ✅ **Filter**: API accepts query params: priority, tags (array), status, due_date_start, due_date_end
- ✅ **Sort**: API accepts sort param: created_at, due_at, priority, title (with asc/desc direction)

### V. Test-Driven Development for Critical Paths (MANDATORY) ✅ COMPLIANT

**Test Coverage**:
- ✅ **Contract Tests**: Pydantic models validate event schemas match spec; tests fail if schema changes break backward compatibility
- ✅ **Integration Tests**: Docker Compose spins up Kafka + PostgreSQL; tests publish events, verify consumers process correctly
- ✅ **Dapr Component Tests**: Tests verify Pub/Sub publishes to correct topic, State API persists data, Jobs API fires at correct time
- ✅ **Failure Scenario Tests**: Tests verify circuit breaker triggers on repeated failures, retries exhaust then DLQ, events reprocessed after broker restart

**TDD Workflow**: Write contract test defining event schema → Run test (fails) → Implement publisher → Run test (passes) → Write integration test for consumer → Run test (fails) → Implement consumer → Run test (passes)

### VI. Observability and Distributed Tracing (MANDATORY) ✅ COMPLIANT

**Logging**:
- ✅ All services use structured JSON logging (library: structlog)
- ✅ Event publications log: event_type, task_id, user_id, timestamp, correlation_id
- ✅ Event consumptions log: event_type, task_id, processing_status, correlation_id, processing_duration_ms
- ✅ Errors log: error_type, error_message, stack_trace, correlation_id

**Metrics**:
- ✅ Prometheus metrics exported via `/metrics` endpoint on each service
- ✅ Event publication rate: `events_published_total{topic, event_type}` counter
- ✅ Event processing latency: `event_processing_duration_seconds{topic, event_type}` histogram
- ✅ Failed events: `event_processing_failures_total{topic, event_type, error_type}` counter
- ✅ Dapr sidecar metrics: `dapr_http_server_request_count`, `dapr_component_loaded` (automatic)

**Tracing**:
- ✅ Dapr automatically injects W3C Trace Context (`traceparent` header) on all HTTP requests
- ✅ Services propagate `traceparent` in outgoing Dapr calls
- ✅ Zipkin collector deployed in Kubernetes, Dapr configured to export spans
- ✅ Traces show: user request → Chat API → Dapr Pub/Sub → Kafka → Dapr subscription → Consumer → Dapr Service Invocation → Chat API

### VII. Infrastructure as Code (MANDATORY) ✅ COMPLIANT

**Directory Structure**:
```
k8s/
├── helm-charts/todo-app/
│   ├── templates/
│   │   ├── deployments/            # chat-api.yaml, recurring-tasks.yaml, notifications.yaml, audit.yaml
│   │   ├── services/               # K8s Services (ClusterIP for internal, LoadBalancer for API)
│   │   ├── ingress/                # NGINX Ingress for external access
│   │   ├── dapr-components/        # kafka-pubsub.yaml, statestore.yaml, secrets.yaml
│   │   ├── kafka/                  # Strimzi Kafka CR (if self-hosted)
│   │   └── configmaps/             # Application config (non-secrets)
│   ├── values.yaml                 # Default values
│   ├── values-minikube.yaml        # Local overrides (NodePort services, resource limits)
│   └── values-cloud.yaml           # Cloud overrides (LoadBalancer services, HPA, PVC)
├── strimzi/                        # Strimzi Kafka operator install manifests
└── monitoring/                     # Prometheus, Grafana, Zipkin manifests
```

**Deployment Process**:
```bash
# 1. Install Strimzi operator (if using self-hosted Kafka)
kubectl create namespace kafka
kubectl apply -f k8s/strimzi/

# 2. Install Dapr on cluster
dapr init -k

# 3. Deploy Kafka cluster (via Strimzi or connect to Redpanda Cloud)
helm install kafka k8s/helm-charts/kafka/ --namespace kafka

# 4. Deploy todo-app services
helm install todo-app k8s/helm-charts/todo-app/ -f k8s/helm-charts/todo-app/values-minikube.yaml
```

**No Manual kubectl Commands**: All resources defined in Helm charts, deployed via `helm install/upgrade`

### VIII. Progressive Deployment Strategy (MANDATORY) ✅ COMPLIANT

**Deployment Gates**:

**Local (Minikube) Gates** ✅:
- Docker images build successfully (< 500MB per image)
- Helm lint passes with zero warnings: `helm lint k8s/helm-charts/todo-app/`
- Dry-run succeeds: `helm install --dry-run --debug todo-app k8s/helm-charts/todo-app/`
- All pods reach Running state within 5 minutes: `kubectl get pods --watch`
- Health checks return 200 OK: `kubectl exec -it <pod> -- curl localhost:8000/health`
- Event flow test passes: Publish task.created event → verify Audit Service logs event received
- Dapr sidecar healthy: `kubectl logs <pod> daprd` shows "status: Running"

**Cloud (AKS/GKE/OKE) Gates** ✅:
- All Minikube gates passed
- Kafka cluster accessible: Test connection from pod to broker
- Secrets configured: Kubernetes Secrets exist for `openai-api-key`, `neon-db-connection-string`
- External database reachable: Test connection to Neon PostgreSQL from cluster
- CI/CD pipeline deploys successfully: GitHub Actions workflow completes
- Load balancer provisions: `kubectl get svc chat-api` shows external IP
- DNS resolves: `nslookup todo-app.example.com` returns external IP
- Monitoring shows healthy metrics: Grafana dashboards display >0 request rate, <200ms latency
- Distributed tracing shows complete traces: Zipkin UI displays end-to-end request spans

**Rollback Requirements** ✅:
- Previous Helm release retained: `helm rollback todo-app <revision>`
- Database migrations backward-compatible: Tested with old app version + new schema
- Event schemas backward-compatible: Consumers handle both old and new event formats (version field in schema)

## Project Structure

### Documentation (this feature)

```text
specs/2-advanced-cloud-deployment/
├── plan.md              # This file (/sp.plan command output)
├── research.md          # Phase 0 output - technology decisions and best practices
├── data-model.md        # Phase 1 output - entities, schemas, relationships
├── quickstart.md        # Phase 1 output - developer setup guide
├── contracts/           # Phase 1 output - API contracts and event schemas
│   ├── openapi.yaml     # REST API specification
│   ├── task-events-schema.json      # TaskEvent schema (Avro or JSON Schema)
│   ├── reminders-schema.json        # ReminderEvent schema
│   └── task-updates-schema.json     # TaskUpdateEvent schema
└── tasks.md             # Phase 2 output (/sp.tasks command - NOT created by /sp.plan)
```

### Source Code (repository root)

**Microservices Architecture**:

```
services/
├── chat-api/                        # Chat API + MCP Tools (FastAPI)
│   ├── src/
│   │   ├── api/                     # REST endpoints
│   │   │   ├── tasks.py             # Task CRUD endpoints
│   │   │   ├── search.py            # Search/filter endpoints
│   │   │   ├── tags.py              # Tag management endpoints
│   │   │   └── health.py            # Health check endpoints
│   │   ├── models/                  # SQLModel entities
│   │   │   ├── task.py              # Task, RecurrencePattern
│   │   │   ├── tag.py               # Tag, TaskTag
│   │   │   └── user.py              # User
│   │   ├── services/                # Business logic
│   │   │   ├── task_service.py      # Task operations + event publishing
│   │   │   ├── search_service.py    # Full-text search logic
│   │   │   └── reminder_service.py  # Dapr Jobs API integration
│   │   ├── events/                  # Event schemas (Pydantic)
│   │   │   ├── task_event.py
│   │   │   ├── reminder_event.py
│   │   │   └── task_update_event.py
│   │   ├── dapr/                    # Dapr client wrappers
│   │   │   ├── pubsub.py            # Publish abstraction
│   │   │   ├── state.py             # State API abstraction
│   │   │   └── secrets.py           # Secrets API abstraction
│   │   └── main.py                  # FastAPI app
│   ├── tests/
│   │   ├── contract/                # Event schema validation tests
│   │   ├── integration/             # End-to-end flow tests
│   │   └── unit/                    # Business logic tests
│   ├── Dockerfile
│   └── requirements.txt
│
├── recurring-tasks/                 # Recurring Task Service (FastAPI)
│   ├── src/
│   │   ├── consumers/               # Dapr subscription handlers
│   │   │   └── task_completed_handler.py
│   │   ├── services/
│   │   │   └── recurrence_calculator.py  # Next due date logic
│   │   └── main.py                  # FastAPI app with /dapr/subscribe endpoint
│   ├── tests/
│   ├── Dockerfile
│   └── requirements.txt
│
├── notifications/                   # Notification Service (FastAPI)
│   ├── src/
│   │   ├── jobs/                    # Dapr Jobs API handlers
│   │   │   └── reminder_trigger.py
│   │   ├── services/
│   │   │   └── notification_dispatcher.py  # Send notifications (email/push)
│   │   └── main.py
│   ├── tests/
│   ├── Dockerfile
│   └── requirements.txt
│
├── audit/                           # Audit Service (FastAPI)
│   ├── src/
│   │   ├── consumers/
│   │   │   └── task_event_handler.py
│   │   ├── models/
│   │   │   └── audit_log.py
│   │   └── main.py
│   ├── tests/
│   ├── Dockerfile
│   └── requirements.txt
│
└── frontend/                        # Next.js 14 frontend
    ├── src/
    │   ├── app/                     # App Router pages
    │   │   ├── tasks/
    │   │   └── layout.tsx
    │   ├── components/              # React components
    │   │   ├── TaskList.tsx
    │   │   ├── TaskForm.tsx
    │   │   ├── SearchBar.tsx
    │   │   └── FilterPanel.tsx
    │   ├── hooks/                   # TanStack Query hooks
    │   │   ├── useTasks.ts
    │   │   ├── useSearch.ts
    │   │   └── useTags.ts
    │   └── lib/
    │       ├── api.ts               # API client (fetch to Chat API)
    │       └── websocket.ts         # Real-time updates via WebSocket
    ├── tests/
    │   └── e2e/                     # Playwright tests
    ├── Dockerfile
    ├── package.json
    └── next.config.js

k8s/                                 # Kubernetes manifests (Helm charts)
├── helm-charts/
│   └── todo-app/
│       ├── templates/
│       ├── values.yaml
│       ├── values-minikube.yaml
│       └── values-cloud.yaml
└── strimzi/                         # Kafka operator

.github/
└── workflows/
    ├── ci.yml                       # Lint, test, build
    └── cd.yml                       # Deploy to staging/production

tests/
├── contract/                        # Cross-service contract tests
├── integration/                     # End-to-end integration tests
└── load/                            # Locust load tests
```

**Structure Decision**: **Microservices architecture selected** because:
1. Multiple independent services required by Constitution II (Chat API, Recurring Tasks, Notifications, Audit)
2. Each service needs separate deployment lifecycle (independent scaling, updates)
3. Enables parallel development by different teams
4. Aligns with event-driven architecture pattern (producers and consumers are separate services)

Alternative considered: Monolithic architecture with async background workers - rejected because violates Constitution II (services must be independently deployable) and doesn't provide fault isolation.

## Complexity Tracking

> **No violations - all constitution principles satisfied**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| N/A | N/A | N/A |

**Notes**:
- Shared PostgreSQL cluster with separate schemas (chat_api, recurring_tasks, notifications, audit) is compliant - services don't share tables, only infrastructure cost optimization
- Dapr sidecar adds complexity but required by Constitution III (infrastructure abstraction) - eliminates tighter coupling to Kafka/database clients
- Four microservices (vs. monolith) justified by Constitution II (independent deployment) and functional separation (user-facing API vs. event consumers)

## Phase 0: Research & Technology Decisions

**Status**: To be generated in `research.md`

**Research Tasks**:

1. **Event Schema Versioning Strategy**: Research how to version event schemas for backward compatibility (Avro schema registry, JSON Schema with version field, Protobuf with field numbers)
2. **Dapr Jobs API Best Practices**: Investigate Dapr Jobs API reliability for reminder scheduling at scale (1000s of concurrent jobs, failure handling, job persistence)
3. **Kafka Topic Partitioning Strategy**: Determine optimal partitioning strategy for `task-events` topic to ensure ordering guarantees per task (partition by task_id) while allowing parallel processing
4. **Full-Text Search Implementation**: Research PostgreSQL full-text search performance with 10,000+ tasks (tsvector indexing, ranking algorithms, query optimization)
5. **Recurring Task Calculation**: Investigate libraries for recurrence pattern calculation (dateutil.rrule, iCalendar RRULE format) to avoid complex date math bugs
6. **Dapr Component Configuration**: Research Dapr component best practices for production (connection pooling, retry policies, timeout configuration for Kafka/PostgreSQL components)
7. **Zero-Downtime Deployment**: Research Helm upgrade strategies for zero-downtime (rolling update pod disruption budgets, readiness gates, pre-stop hooks)
8. **Distributed Tracing Sampling**: Determine optimal tracing sampling rate for production (100% trace all requests vs. sample 10% to reduce overhead)
9. **Event Dead Letter Queue**: Research Dapr/Kafka DLQ pattern for failed event processing (max retries, DLQ topic configuration, monitoring/alerting for DLQ depth)
10. **Kubernetes Autoscaling**: Research HPA (Horizontal Pod Autoscaler) configuration for event-driven workloads (metrics: CPU, memory, or custom metrics like Kafka consumer lag)

**Output**: `research.md` will document decisions, rationale, and alternatives for each research task.

## Phase 1: Design & Contracts

**Status**: To be generated in Phase 1

**Deliverables**:

1. **data-model.md**:
   - Task entity (id, user_id, title, description, status, priority, due_at, created_at, updated_at, completed_at)
   - RecurrencePattern entity (task_id, pattern_type, interval, days_of_week, day_of_month, end_condition, end_date, occurrence_count)
   - Tag entity (id, name, usage_count)
   - TaskTag junction (task_id, tag_id)
   - Reminder entity (task_id, remind_at, delivery_status, notification_channel)
   - AuditLog entity (id, event_type, task_id, user_id, event_data, timestamp, correlation_id)
   - State transitions (task status: todo → in_progress → complete; reminder status: pending → sent → cancelled)

2. **contracts/openapi.yaml**:
   - POST /tasks (create task with optional recurrence and reminders)
   - GET /tasks (list with pagination, filter by priority/tags/status/due date range, sort by various fields, search query)
   - GET /tasks/{id} (get single task with full details)
   - PATCH /tasks/{id} (update task fields)
   - DELETE /tasks/{id} (delete task or single instance of recurring task)
   - POST /tasks/{id}/complete (mark complete, triggers recurring task generation)
   - GET /tags (list all tags with usage count for autocomplete)
   - POST /tags (create new tag)
   - GET /health (health check)
   - GET /ready (readiness check)

3. **contracts/task-events-schema.json**:
   - TaskEvent schema (version 1.0): event_type (created/updated/completed/deleted), task_id, user_id, timestamp, correlation_id, task_snapshot (full Task object)

4. **contracts/reminders-schema.json**:
   - ReminderEvent schema (version 1.0): task_id, title, due_at, remind_at, user_id, correlation_id

5. **contracts/task-updates-schema.json**:
   - TaskUpdateEvent schema (version 1.0): task_id, change_summary, user_id, timestamp, correlation_id

6. **quickstart.md**:
   - Prerequisites (Docker, Minikube, kubectl, Helm, Dapr CLI)
   - Local development setup steps
   - How to run services locally
   - How to deploy to Minikube
   - How to run tests
   - Troubleshooting guide

7. **Agent context update**:
   - Run `.specify/scripts/powershell/update-agent-context.ps1 -AgentType claude`
   - Add technology stack to `.claude/CLAUDE.md` or equivalent agent context file
   - Preserve manual context between markers

## Phase 2: Task Breakdown

**Status**: Not generated by `/sp.plan` - requires separate `/sp.tasks` command

**Expected Output**: `tasks.md` will contain implementation tasks organized by user story, dependencies, and parallelization opportunities, following constitution-driven TDD workflow (write tests → implement → verify).

## Next Steps

After `/sp.plan` completes:

1. **Immediate**: Run `/sp.tasks` to generate actionable task breakdown
2. **Before Implementation**: Create ADRs for:
   - Event Schema Design (version strategy, schema format)
   - Dapr Component Selection (Kafka vs. Redpanda, State store configuration)
   - Kafka Topic Strategy (partitioning, replication factor, retention policy)
   - Service Boundaries (which functionality belongs in which service)
3. **Implementation**: Follow Agentic Dev Stack workflow: `/sp.implement` to execute tasks
4. **Review**: Run `/sp.analyze` for cross-artifact consistency check
5. **Production Readiness**: Validate all constitution gates passed, monitoring configured, rollback tested

---

**Plan Status**: ✅ COMPLETE - Ready for Phase 0 research
**Constitution Compliance**: ✅ ALL GATES PASSED - No violations
**Next Command**: Generate research findings with research agents, then proceed to Phase 1 design
